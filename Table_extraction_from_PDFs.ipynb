{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I65bvmvwh-SS"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G57zl8IEiJC-"
      },
      "outputs": [],
      "source": [
        "!pip install paddlepaddle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJwm5jo_iMdH"
      },
      "source": [
        "#Converting PDF to images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ6U3U7JiLmc"
      },
      "outputs": [],
      "source": [
        "!pip install pdf2image\n",
        "!apt-get update\n",
        "!apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzKsFrD0iPRv"
      },
      "outputs": [],
      "source": [
        "from pdf2image import convert_from_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4WFItjCiS6n"
      },
      "outputs": [],
      "source": [
        "images = convert_from_path('/content/j.ijhydene.2014.06.160.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Mh9LMzCkSsv"
      },
      "outputs": [],
      "source": [
        "!mkdir pages_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0M9GC9oiUIV"
      },
      "outputs": [],
      "source": [
        "total_page = 0\n",
        "for i in range(len(images)):\n",
        "  images[i].save('pages_1/page'+str(i)+'.jpg', 'JPEG')\n",
        "  total_page +=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5Go3zuAiWrC",
        "outputId": "404ca9e8-9683-4aab-908e-7a1742d36f65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_page"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOwLyipQijQ7"
      },
      "source": [
        "#getting tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QS9HWHvib_U"
      },
      "outputs": [],
      "source": [
        "#!python3 -m pip install paddlepaddle-gpu\n",
        "!pip install \"paddleocr>=2.0.1\"\n",
        "!pip install protobuf\n",
        "!git clone https://github.com/PaddlePaddle/PaddleOCR.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHNDBV7tieOn"
      },
      "outputs": [],
      "source": [
        "!wget https://paddleocr.bj.bcebos.com/whl/layoutparser-0.0.0-py3-none-any.whl\n",
        "!pip install -U layoutparser-0.0.0-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHMYOLdpFwXj"
      },
      "outputs": [],
      "source": [
        "!wget http://archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.0g-2ubuntu4_amd64.deb\n",
        "!sudo dpkg -i libssl1.1_1.1.0g-2ubuntu4_amd64.deb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bh0ufuJlmw_1"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade paddleocr\n",
        "!pip install --upgrade paddlepaddle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzffwE8Sigzi"
      },
      "outputs": [],
      "source": [
        "from paddleocr import PaddleOCR, draw_ocr\n",
        "import cv2\n",
        "import layoutparser as lp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBqgY_Psims3",
        "outputId": "2b76d27e-cb23-4e79-9df3-ade39ac4ec38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "download https://paddle-model-ecology.bj.bcebos.com/model/layout-parser/ppyolov2_r50vd_dcn_365e_publaynet.tar to /root/.paddledet/inference_model/ppyolov2_r50vd_dcn_365e_publaynet/ppyolov2_r50vd_dcn_365e_publaynet_infer/ppyolov2_r50vd_dcn_365e_publaynet.tar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 221M/221M [00:31<00:00, 7.09MiB/s]\n"
          ]
        }
      ],
      "source": [
        "# load model\n",
        "model = lp.PaddleDetectionLayoutModel(config_path=\"lp://PubLayNet/ppyolov2_r50vd_dcn_365e_publaynet/config\",\n",
        "                                threshold=0.5,\n",
        "                                label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"},\n",
        "                                enforce_cpu=False,\n",
        "                                enable_mkldnn=True)#math kernel library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8pl62tsvRg8"
      },
      "outputs": [],
      "source": [
        "def intersection(box_1, box_2):\n",
        "  return [box_2[0], box_1[1],box_2[2], box_1[3]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCHlHWlpvUOz"
      },
      "outputs": [],
      "source": [
        "def iou(box_1, box_2):\n",
        "\n",
        "  x_1 = max(box_1[0], box_2[0])\n",
        "  y_1 = max(box_1[1], box_2[1])\n",
        "  x_2 = min(box_1[2], box_2[2])\n",
        "  y_2 = min(box_1[3], box_2[3])\n",
        "\n",
        "  inter = abs(max((x_2 - x_1, 0)) * max((y_2 - y_1), 0))\n",
        "  if inter == 0:\n",
        "      return 0\n",
        "\n",
        "  box_1_area = abs((box_1[2] - box_1[0]) * (box_1[3] - box_1[1]))\n",
        "  box_2_area = abs((box_2[2] - box_2[0]) * (box_2[3] - box_2[1]))\n",
        "\n",
        "  return inter / float(box_1_area + box_2_area - inter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5gD2Tawiuh5",
        "outputId": "e32e9be9-6d97-4269-b562-c1ec59d38ada"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "130 230 1512 417\n",
            "download https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar to /root/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer/en_PP-OCRv3_det_infer.tar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.00M/4.00M [00:16<00:00, 249kiB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "download https://paddleocr.bj.bcebos.com/PP-OCRv4/english/en_PP-OCRv4_rec_infer.tar to /root/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer/en_PP-OCRv4_rec_infer.tar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10.2M/10.2M [00:15<00:00, 644kiB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "download https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar to /root/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer/ch_ppocr_mobile_v2.0_cls_infer.tar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2.19M/2.19M [00:13<00:00, 157kiB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023/08/08 15:29:17] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/root/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/root/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/usr/local/lib/python3.10/dist-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=False, cls_model_dir='/root/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, lang='en', det=True, rec=True, type='ocr', ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Page number is 3\n",
            "Current Table count is  0\n",
            "130 230 1512 417\n",
            "[2023/08/08 15:29:17] ppocr WARNING: Since the angle classifier is not initialized, the angle classifier will not be uesd during the forward process\n",
            "[2023/08/08 15:29:17] ppocr DEBUG: dt_boxes num : 48, elapse : 0.12746357917785645\n",
            "[2023/08/08 15:29:25] ppocr DEBUG: rec_res num  : 48, elapse : 7.115644454956055\n"
          ]
        }
      ],
      "source": [
        "for num in range(total_page):\n",
        "  image = cv2.imread('/content/pages_1/page'+str(num)+'.jpg')\n",
        "\n",
        "  image = image[..., ::-1]\n",
        "\n",
        "  # detect\n",
        "  layout = model.detect(image)\n",
        "\n",
        "  counts=0\n",
        "  for l in layout:\n",
        "      if l.type == 'Table':\n",
        "        counts +=1\n",
        "  if counts!=0:\n",
        "    x_1=[0]*counts\n",
        "    y_1=[0]*counts\n",
        "    x_2=[0]*counts\n",
        "    y_2=[0]*counts\n",
        "    i=0\n",
        "    for l in layout:\n",
        "      #print(l)\n",
        "      if l.type == 'Table':\n",
        "        x_1[i] = int(l.block.x_1)\n",
        "        #print(l.block.x_1)\n",
        "        y_1[i] = int(l.block.y_1)\n",
        "        x_2[i] = int(l.block.x_2)\n",
        "        y_2[i] = int(l.block.y_2)\n",
        "        print(x_1[i],y_1[i],x_2[i],y_2[i])\n",
        "        i+=1\n",
        "\n",
        "\n",
        "    im = cv2.imread('/content/pages_1/page'+str(num)+'.jpg')\n",
        "    ocr = PaddleOCR(lang='en')\n",
        "    for count in range(counts):\n",
        "      print(\"Current Page number is\",num)\n",
        "      print(\"Current Table count is \",count)\n",
        "      print(x_1[count],y_1[count],x_2[count],y_2[count])\n",
        "      cv2.imwrite('ext_im'+str(num)+str(count+1)+'.jpg', im[y_1[count]:y_2[count],x_1[count]:x_2[count]])\n",
        "\n",
        "\n",
        "      image_path = '/content/ext_im'+str(num)+str(count+1)+'.jpg'\n",
        "      image_cv = cv2.imread(image_path)\n",
        "      image_height = image_cv.shape[0]\n",
        "      image_width = image_cv.shape[1]\n",
        "      output = ocr.ocr(image_path)[0]\n",
        "\n",
        "      boxes = [line[0] for line in output]\n",
        "      texts = [line[1][0] for line in output]\n",
        "      probabilities = [line[1][1] for line in output]\n",
        "\n",
        "      image_boxes = image_cv.copy()\n",
        "\n",
        "      for box,text in zip(boxes,texts):\n",
        "        cv2.rectangle(image_boxes, (int(box[0][0]),int(box[0][1])), (int(box[2][0]),int(box[2][1])),(0,0,255),1)\n",
        "        cv2.putText(image_boxes, text,(int(box[0][0]),int(box[0][1])),cv2.FONT_HERSHEY_SIMPLEX,1,(222,0,0),1)\n",
        "\n",
        "      cv2.imwrite('detections'+str(num)+str(count+1)+'.jpg', image_boxes)\n",
        "\n",
        "    #get horizontal and vertical lines\n",
        "      imx = image_cv.copy()\n",
        "\n",
        "      horiz_boxes = []\n",
        "      vert_boxes = []\n",
        "\n",
        "      for box in boxes:\n",
        "        x_h, x_v = 0,int(box[0][0])\n",
        "        y_h, y_v = int(box[0][1]),0\n",
        "        width_h,width_v = image_width, int(box[2][0]-box[0][0])\n",
        "        height_h,height_v = int(box[2][1]-box[0][1]),image_height\n",
        "\n",
        "        horiz_boxes.append([x_h,y_h,x_h+width_h,y_h+height_h])\n",
        "        vert_boxes.append([x_v,y_v,x_v+width_v,y_v+height_v])\n",
        "\n",
        "        cv2.rectangle(imx,(x_h,y_h), (x_h+width_h,y_h+height_h),(0,0,255),1)\n",
        "        cv2.rectangle(imx,(x_v,y_v), (x_v+width_v,y_v+height_v),(0,255,0),1)\n",
        "\n",
        "      #cv2.imwrite('horiz_vert'+str(num)+str(count+1)+'.jpg',imx)\n",
        "\n",
        "      #Non Max Suppresion\n",
        "      horiz_out = tf.image.non_max_suppression(\n",
        "        horiz_boxes,\n",
        "        probabilities,\n",
        "        max_output_size = 1000,\n",
        "        iou_threshold=0.1,\n",
        "        score_threshold=float('-inf'),\n",
        "        name=None\n",
        "      )\n",
        "      horiz_lines = np.sort(np.array(horiz_out))\n",
        "      im_nms = image_cv.copy()\n",
        "      for val in horiz_lines:\n",
        "        cv2.rectangle(im_nms, (int(horiz_boxes[val][0]),int(horiz_boxes[val][1])), (int(horiz_boxes[val][2]),int(horiz_boxes[val][3])),(0,0,255),1)\n",
        "\n",
        "      #cv2.imwrite('im_nms_h'+str(num)+str(count+1)+'.jpg',im_nms)\n",
        "\n",
        "      vert_out = tf.image.non_max_suppression(\n",
        "        vert_boxes,\n",
        "        probabilities,\n",
        "        max_output_size = 1000,\n",
        "        iou_threshold=0.1,\n",
        "        score_threshold=float('-inf'),\n",
        "        name=None\n",
        "      )\n",
        "      vert_lines = np.sort(np.array(vert_out))\n",
        "      im_nms = image_cv.copy()\n",
        "\n",
        "      for val in vert_lines:\n",
        "        cv2.rectangle(im_nms, (int(vert_boxes[val][0]),int(vert_boxes[val][1])), (int(vert_boxes[val][2]),int(vert_boxes[val][3])),(255,0,0),1)\n",
        "\n",
        "      #cv2.imwrite('im_nms_v'+str(num)+str(count+1)+'.jpg',im_nms)\n",
        "\n",
        "    #Convert to CSV\n",
        "      out_array = [[\"\" for i in range(len(vert_lines))] for j in range(len(horiz_lines))]\n",
        "      # print(np.array(out_array).shape)\n",
        "      # print(out_array)\n",
        "\n",
        "      unordered_boxes = []\n",
        "\n",
        "      for i in vert_lines:\n",
        "        #print(vert_boxes[i])\n",
        "        unordered_boxes.append(vert_boxes[i][0])\n",
        "\n",
        "      ordered_boxes = np.argsort(unordered_boxes)\n",
        "\n",
        "      for i in range(len(horiz_lines)):\n",
        "        for j in range(len(vert_lines)):\n",
        "          resultant = intersection(horiz_boxes[horiz_lines[i]], vert_boxes[vert_lines[ordered_boxes[j]]] )\n",
        "\n",
        "          for b in range(len(boxes)):\n",
        "            the_box = [boxes[b][0][0],boxes[b][0][1],boxes[b][2][0],boxes[b][2][1]]\n",
        "            if(iou(resultant,the_box)>0.1):\n",
        "              out_array[i][j] = texts[b]\n",
        "\n",
        "      out_array=np.array(out_array)\n",
        "      pd.DataFrame(out_array).to_csv('/content/Sheet_'+str(num)+str(count+1)+'.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aDV3c0klGLR",
        "outputId": "aac809cd-3c51-409e-8cca-80aad02ac8bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZScCNV8v-QiK"
      },
      "source": [
        "#New"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2Zb7giK_5GL"
      },
      "source": [
        "##Mounting drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoJR8bw08yUg",
        "outputId": "df510b6d-b8ce-45f3-e5f8-ba849f859c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AOiagSS__2R"
      },
      "source": [
        "##importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZYcEi44-f-G"
      },
      "outputs": [],
      "source": [
        "#general libraries\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbrZDwRr-Z73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be03a817-3c4a-4349-f38d-b2d222bc9049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.16.3\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [980 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,136 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [860 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,241 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 4,558 kB in 2s (2,222 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.2 [186 kB]\n",
            "Fetched 186 kB in 0s (595 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 120831 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.2) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "#For pdf2image\n",
        "!pip install pdf2image\n",
        "!apt-get update\n",
        "!apt-get install poppler-utils\n",
        "from pdf2image import convert_from_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1JUb5TyAcZ1"
      },
      "source": [
        "###paddlepaddle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cp5hEN6lAYy9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1f5a8c8-d85e-433a-e284-b31cd29ab65f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting paddlepaddle-gpu\n",
            "  Downloading paddlepaddle_gpu-2.5.1-cp310-cp310-manylinux1_x86_64.whl (541.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.0/541.0 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from paddlepaddle-gpu)\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu) (9.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu) (4.4.2)\n",
            "Collecting astor (from paddlepaddle-gpu)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting paddle-bfloat==0.1.7 (from paddlepaddle-gpu)\n",
            "  Downloading paddle_bfloat-0.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (383 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu) (3.20.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle-gpu) (2023.7.22)\n",
            "Collecting httpcore<0.18.0,>=0.15.0 (from httpx->paddlepaddle-gpu)\n",
            "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle-gpu) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle-gpu) (1.3.0)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore<0.18.0,>=0.15.0->httpx->paddlepaddle-gpu)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->paddlepaddle-gpu) (3.7.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->paddlepaddle-gpu) (1.1.3)\n",
            "Installing collected packages: paddle-bfloat, h11, astor, httpcore, httpx, paddlepaddle-gpu\n",
            "Successfully installed astor-0.8.1 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 paddle-bfloat-0.1.7 paddlepaddle-gpu-2.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install paddlepaddle-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBw0ZNC4-Po3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ebdb5191-9dcb-4c16-cc1d-2314f1807389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting paddleocr>=2.0.1\n",
            "  Downloading paddleocr-2.7.0.2-py3-none-any.whl (466 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.1/466.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from paddleocr>=2.0.1) (2.0.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from paddleocr>=2.0.1) (0.19.3)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.10/dist-packages (from paddleocr>=2.0.1) (0.4.0)\n",
            "Collecting pyclipper (from paddleocr>=2.0.1)\n",
            "  Downloading pyclipper-1.3.0.post4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (813 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m813.9/813.9 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lmdb (from paddleocr>=2.0.1)\n",
            "  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from paddleocr>=2.0.1) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from paddleocr>=2.0.1) (1.23.5)\n",
            "Collecting visualdl (from paddleocr>=2.0.1)\n",
            "  Downloading visualdl-2.5.3-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz (from paddleocr>=2.0.1)\n",
            "  Downloading rapidfuzz-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python<=4.6.0.66 (from paddleocr>=2.0.1)\n",
            "  Downloading opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-contrib-python<=4.6.0.66 (from paddleocr>=2.0.1)\n",
            "  Downloading opencv_contrib_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from paddleocr>=2.0.1) (0.29.36)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from paddleocr>=2.0.1) (4.9.3)\n",
            "Collecting premailer (from paddleocr>=2.0.1)\n",
            "  Downloading premailer-3.10.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from paddleocr>=2.0.1) (3.1.2)\n",
            "Collecting attrdict (from paddleocr>=2.0.1)\n",
            "  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Collecting PyMuPDF<1.21.0 (from paddleocr>=2.0.1)\n",
            "  Downloading PyMuPDF-1.20.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from paddleocr>=2.0.1) (9.4.0)\n",
            "Collecting python-docx (from paddleocr>=2.0.1)\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from paddleocr>=2.0.1) (4.11.2)\n",
            "Requirement already satisfied: fonttools>=4.24.0 in /usr/local/lib/python3.10/dist-packages (from paddleocr>=2.0.1) (4.42.0)\n",
            "Collecting fire>=0.3.0 (from paddleocr>=2.0.1)\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdf2docx (from paddleocr>=2.0.1)\n",
            "  Downloading pdf2docx-0.5.6-py3-none-any.whl (148 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.4/148.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire>=0.3.0->paddleocr>=2.0.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire>=0.3.0->paddleocr>=2.0.1) (2.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->paddleocr>=2.0.1) (2.4.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imgaug->paddleocr>=2.0.1) (1.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imgaug->paddleocr>=2.0.1) (3.7.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug->paddleocr>=2.0.1) (2.31.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->paddleocr>=2.0.1) (3.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->paddleocr>=2.0.1) (2023.8.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->paddleocr>=2.0.1) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->paddleocr>=2.0.1) (23.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->paddleocr>=2.0.1) (1.1.0)\n",
            "Collecting cssselect (from premailer->paddleocr>=2.0.1)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting cssutils (from premailer->paddleocr>=2.0.1)\n",
            "  Downloading cssutils-2.7.1-py3-none-any.whl (399 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.7/399.7 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from premailer->paddleocr>=2.0.1) (2.31.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from premailer->paddleocr>=2.0.1) (5.3.1)\n",
            "Collecting bce-python-sdk (from visualdl->paddleocr>=2.0.1)\n",
            "  Downloading bce_python_sdk-0.8.90-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.3/235.3 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flask>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from visualdl->paddleocr>=2.0.1) (2.2.5)\n",
            "Collecting Flask-Babel>=3.0.0 (from visualdl->paddleocr>=2.0.1)\n",
            "  Downloading flask_babel-3.1.0-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.10/dist-packages (from visualdl->paddleocr>=2.0.1) (3.20.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from visualdl->paddleocr>=2.0.1) (1.5.3)\n",
            "Collecting rarfile (from visualdl->paddleocr>=2.0.1)\n",
            "  Downloading rarfile-4.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from visualdl->paddleocr>=2.0.1) (5.9.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask>=1.1.1->visualdl->paddleocr>=2.0.1) (2.3.7)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask>=1.1.1->visualdl->paddleocr>=2.0.1) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask>=1.1.1->visualdl->paddleocr>=2.0.1) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask>=1.1.1->visualdl->paddleocr>=2.0.1) (8.1.7)\n",
            "Requirement already satisfied: Babel>=2.12 in /usr/local/lib/python3.10/dist-packages (from Flask-Babel>=3.0.0->visualdl->paddleocr>=2.0.1) (2.12.1)\n",
            "Requirement already satisfied: pytz>=2022.7 in /usr/local/lib/python3.10/dist-packages (from Flask-Babel>=3.0.0->visualdl->paddleocr>=2.0.1) (2023.3)\n",
            "Collecting pycryptodome>=3.8.0 (from bce-python-sdk->visualdl->paddleocr>=2.0.1)\n",
            "  Downloading pycryptodome-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from bce-python-sdk->visualdl->paddleocr>=2.0.1) (0.18.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->paddleocr>=2.0.1) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->paddleocr>=2.0.1) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->paddleocr>=2.0.1) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->paddleocr>=2.0.1) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->paddleocr>=2.0.1) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->premailer->paddleocr>=2.0.1) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->premailer->paddleocr>=2.0.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->premailer->paddleocr>=2.0.1) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->premailer->paddleocr>=2.0.1) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask>=1.1.1->visualdl->paddleocr>=2.0.1) (2.1.3)\n",
            "Building wheels for collected packages: fire, python-docx\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=2dcb99556f38b3f44167b7df425a600c43bff732f90b8911f3a22c58e050e0a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184487 sha256=d81e313d7a38c2c623cc5cf09fdec69a802b9b1fd78a5d492c95a99a75116bd8\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
            "Successfully built fire python-docx\n",
            "Installing collected packages: rarfile, pyclipper, lmdb, rapidfuzz, python-docx, PyMuPDF, pycryptodome, opencv-python, opencv-contrib-python, fire, cssutils, cssselect, attrdict, premailer, pdf2docx, bce-python-sdk, Flask-Babel, visualdl, paddleocr\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.8.0.76\n",
            "    Uninstalling opencv-python-4.8.0.76:\n",
            "      Successfully uninstalled opencv-python-4.8.0.76\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.8.0.76\n",
            "    Uninstalling opencv-contrib-python-4.8.0.76:\n",
            "      Successfully uninstalled opencv-contrib-python-4.8.0.76\n",
            "Successfully installed Flask-Babel-3.1.0 PyMuPDF-1.20.2 attrdict-2.0.1 bce-python-sdk-0.8.90 cssselect-1.2.0 cssutils-2.7.1 fire-0.5.0 lmdb-1.4.1 opencv-contrib-python-4.6.0.66 opencv-python-4.6.0.66 paddleocr-2.7.0.2 pdf2docx-0.5.6 premailer-3.10.0 pyclipper-1.3.0.post4 pycryptodome-3.18.0 python-docx-0.8.11 rapidfuzz-3.2.0 rarfile-4.0 visualdl-2.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (3.20.3)\n",
            "Cloning into 'PaddleOCR'...\n",
            "remote: Enumerating objects: 47048, done.\u001b[K\n",
            "remote: Counting objects: 100% (292/292), done.\u001b[K\n",
            "remote: Compressing objects: 100% (172/172), done.\u001b[K\n",
            "remote: Total 47048 (delta 169), reused 207 (delta 120), pack-reused 46756\u001b[K\n",
            "Receiving objects: 100% (47048/47048), 343.32 MiB | 30.29 MiB/s, done.\n",
            "Resolving deltas: 100% (33019/33019), done.\n",
            "Updating files: 100% (1889/1889), done.\n"
          ]
        }
      ],
      "source": [
        "#For paddlepaddle\n",
        "!pip install \"paddleocr>=2.0.1\"\n",
        "!pip install protobuf\n",
        "!git clone https://github.com/PaddlePaddle/PaddleOCR.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzKu0OsR_Ug3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf5bdcb5-f863-4451-d046-181599e81e1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-22 11:59:10--  https://paddleocr.bj.bcebos.com/whl/layoutparser-0.0.0-py3-none-any.whl\n",
            "Resolving paddleocr.bj.bcebos.com (paddleocr.bj.bcebos.com)... 103.235.46.61, 2409:8c04:1001:1002:0:ff:b001:368a\n",
            "Connecting to paddleocr.bj.bcebos.com (paddleocr.bj.bcebos.com)|103.235.46.61|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19145360 (18M) [application/octet-stream]\n",
            "Saving to: ‘layoutparser-0.0.0-py3-none-any.whl’\n",
            "\n",
            "layoutparser-0.0.0- 100%[===================>]  18.26M  3.31MB/s    in 17s     \n",
            "\n",
            "2023-08-22 11:59:28 (1.09 MB/s) - ‘layoutparser-0.0.0-py3-none-any.whl’ saved [19145360/19145360]\n",
            "\n",
            "Processing ./layoutparser-0.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from layoutparser==0.0.0) (1.23.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from layoutparser==0.0.0) (4.6.0.66)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from layoutparser==0.0.0) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from layoutparser==0.0.0) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from layoutparser==0.0.0) (6.0.1)\n",
            "Collecting iopath (from layoutparser==0.0.0)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m701.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from layoutparser==0.0.0) (4.66.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath->layoutparser==0.0.0) (4.7.1)\n",
            "Collecting portalocker (from iopath->layoutparser==0.0.0)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser==0.0.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser==0.0.0) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->layoutparser==0.0.0) (1.16.0)\n",
            "Building wheels for collected packages: iopath\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31530 sha256=4e46c708a02388c8e74d38e0160692f9e89767e0cef7787eba5d9f2c7e50609a\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built iopath\n",
            "Installing collected packages: portalocker, iopath, layoutparser\n",
            "Successfully installed iopath-0.1.10 layoutparser-0.0.0 portalocker-2.7.0\n"
          ]
        }
      ],
      "source": [
        "!wget https://paddleocr.bj.bcebos.com/whl/layoutparser-0.0.0-py3-none-any.whl\n",
        "!pip install -U layoutparser-0.0.0-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJJbOszq_IRu"
      },
      "outputs": [],
      "source": [
        "!wget http://archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.0g-2ubuntu4_amd64.deb\n",
        "!sudo dpkg -i libssl1.1_1.1.0g-2ubuntu4_amd64.deb\n",
        "!pip install --upgrade paddleocr\n",
        "!pip install --upgrade paddlepaddle-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnQ_h927AQ-U"
      },
      "outputs": [],
      "source": [
        "from pdf2image import convert_from_path\n",
        "from paddleocr import PaddleOCR, draw_ocr\n",
        "import cv2\n",
        "import layoutparser as lp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSWbCRJeBA0c"
      },
      "source": [
        "##Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vkx59bXe97ac"
      },
      "outputs": [],
      "source": [
        "# Paths\n",
        "pdf_folder = '/content/drive/MyDrive/5th Year (2023)/AmritanshuKhareDualDegree2024/MTP_Dataset_todo'  # Replace with your PDFs folder path\n",
        "output_folder = '/content/drive/MyDrive/5th Year (2023)/AmritanshuKhareDualDegree2024/Test_Output_6'  # Replace with desired output folder path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwxgWptL_tMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fc0a55f-2e33-4737-be00-1a37b447bf7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download https://paddle-model-ecology.bj.bcebos.com/model/layout-parser/ppyolov2_r50vd_dcn_365e_publaynet.tar to /root/.paddledet/inference_model/ppyolov2_r50vd_dcn_365e_publaynet/ppyolov2_r50vd_dcn_365e_publaynet_infer/ppyolov2_r50vd_dcn_365e_publaynet.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 221M/221M [00:28<00:00, 7.80MiB/s]\n"
          ]
        }
      ],
      "source": [
        "# load model\n",
        "model = lp.PaddleDetectionLayoutModel(config_path=\"lp://PubLayNet/ppyolov2_r50vd_dcn_365e_publaynet/config\",\n",
        "                                threshold=0.6,\n",
        "                                label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"},\n",
        "                                enforce_cpu=False,\n",
        "                                enable_mkldnn=True)#math kernel library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DwLpDQ1Bik3"
      },
      "source": [
        "###Processing each pdf file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def intersection(box_1, box_2):\n",
        "  return [box_2[0], box_1[1],box_2[2], box_1[3]]"
      ],
      "metadata": {
        "id": "oFpxjHT0Gns9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iou(box_1, box_2):\n",
        "\n",
        "  x_1 = max(box_1[0], box_2[0])\n",
        "  y_1 = max(box_1[1], box_2[1])\n",
        "  x_2 = min(box_1[2], box_2[2])\n",
        "  y_2 = min(box_1[3], box_2[3])\n",
        "\n",
        "  inter = abs(max((x_2 - x_1, 0)) * max((y_2 - y_1), 0))\n",
        "  if inter == 0:\n",
        "      return 0\n",
        "\n",
        "  box_1_area = abs((box_1[2] - box_1[0]) * (box_1[3] - box_1[1]))\n",
        "  box_2_area = abs((box_2[2] - box_2[0]) * (box_2[3] - box_2[1]))\n",
        "\n",
        "  return inter / float(box_1_area + box_2_area - inter)"
      ],
      "metadata": {
        "id": "nKjkfI2dGonx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v67t0RyjBlE_"
      },
      "outputs": [],
      "source": [
        "pdf_files = os.listdir(pdf_folder)\n",
        "for pdf_file in pdf_files:\n",
        "    if pdf_file.endswith('.pdf'):\n",
        "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
        "        pdf_name = os.path.splitext(pdf_file)[0]\n",
        "        print(pdf_name)\n",
        "        # Create a directory for PDF-specific data\n",
        "        pdf_output_dir = os.path.join(output_folder, pdf_name)\n",
        "        os.makedirs(pdf_output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "        # Convert PDF pages to images\n",
        "        pdf_images = convert_from_path(pdf_path)\n",
        "        page_dir = os.path.join(pdf_output_dir, pdf_name + '_pages')\n",
        "        os.makedirs(page_dir, exist_ok=True)\n",
        "\n",
        "        total_pages=0\n",
        "\n",
        "        for page_num, image in enumerate(pdf_images):\n",
        "            page_image_path = os.path.join(page_dir, f'page{page_num + 1}.jpg')\n",
        "            image.save(page_image_path, 'JPEG')\n",
        "            total_pages+=1\n",
        "\n",
        "            image = cv2.imread(page_image_path)\n",
        "            image = image[..., ::-1]\n",
        "            layout = model.detect(image)\n",
        "            counts=0\n",
        "            for l in layout:\n",
        "                if l.type == 'Table':\n",
        "                  counts +=1\n",
        "            #print(\"for page no.\", page_num+1,\" the table_count = \",counts)\n",
        "\n",
        "\n",
        "            #if tables are present in this page\n",
        "            if counts!=0:\n",
        "              #print(\"counts\")\n",
        "\n",
        "              # Create a directory for extracted table images\n",
        "              table_img_dir = os.path.join(pdf_output_dir, 'table_imgs')\n",
        "              os.makedirs(table_img_dir, exist_ok=True)\n",
        "\n",
        "              x_1=[0]*counts\n",
        "              y_1=[0]*counts\n",
        "              x_2=[0]*counts\n",
        "              y_2=[0]*counts\n",
        "              i=0\n",
        "              for l in layout:\n",
        "                #print(l)\n",
        "                if l.type == 'Table':\n",
        "                  x_1[i] = int(l.block.x_1)\n",
        "                  #print(l.block.x_1)\n",
        "                  y_1[i] = int(l.block.y_1)\n",
        "                  x_2[i] = int(l.block.x_2)\n",
        "                  y_2[i] = int(l.block.y_2)\n",
        "                  print(x_1[i],y_1[i],x_2[i],y_2[i])\n",
        "                  i+=1\n",
        "\n",
        "              ocr = PaddleOCR(lang='en')\n",
        "              for count in range(counts):\n",
        "                  # Extract the table image\n",
        "                table_image = image[y_1[count]:y_2[count], x_1[count]:x_2[count]]\n",
        "\n",
        "                  # Save the table image with the specified naming format\n",
        "                table_img_path = os.path.join(table_img_dir, f'table_img_{page_num + 1}_{count + 1}.jpg')\n",
        "                cv2.imwrite(table_img_path, table_image)\n",
        "                #print(\"this ran\")\n",
        "\n",
        "\n",
        "              # Iterate through table images in 'table_imgs' directory\n",
        "              table_img_dir = os.path.join(pdf_output_dir, 'table_imgs')\n",
        "              table_images = [f for f in os.listdir(table_img_dir) if f.endswith('.jpg')]\n",
        "\n",
        "              for table_img_file in table_images:\n",
        "                  # Skip detection images\n",
        "                  if table_img_file.startswith('detections_'):\n",
        "                      continue\n",
        "\n",
        "                  table_img_path = os.path.join(table_img_dir, table_img_file)\n",
        "                  table_image_cv = cv2.imread(table_img_path)\n",
        "                  image_height = table_image_cv.shape[0]\n",
        "                  image_width = table_image_cv.shape[1]\n",
        "\n",
        "                  # Perform OCR on the table image\n",
        "                  output = ocr.ocr(table_img_path)[0]\n",
        "                  boxes = [line[0] for line in output]\n",
        "                  texts = [line[1][0] for line in output]\n",
        "                  probabilities = [line[1][1] for line in output]\n",
        "\n",
        "                  image_boxes = table_image_cv.copy()\n",
        "\n",
        "                  for box, text in zip(boxes, texts):\n",
        "                      cv2.rectangle(image_boxes, (int(box[0][0]), int(box[0][1])), (int(box[2][0]), int(box[2][1])), (0, 0, 255), 1)\n",
        "                      cv2.putText(image_boxes, text, (int(box[0][0]), int(box[0][1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (222, 0, 0), 1)\n",
        "\n",
        "                  detections_img_path = os.path.join(table_img_dir, f'detections_{table_img_file}')\n",
        "                  cv2.imwrite(detections_img_path, image_boxes)\n",
        "\n",
        "              imx = table_image_cv.copy()\n",
        "\n",
        "              horiz_boxes = []\n",
        "              vert_boxes = []\n",
        "\n",
        "              for box in boxes:\n",
        "                x_h, x_v = 0,int(box[0][0])\n",
        "                y_h, y_v = int(box[0][1]),0\n",
        "                width_h,width_v = image_width, int(box[2][0]-box[0][0])\n",
        "                height_h,height_v = int(box[2][1]-box[0][1]),image_height\n",
        "\n",
        "                horiz_boxes.append([x_h,y_h,x_h+width_h,y_h+height_h])\n",
        "                vert_boxes.append([x_v,y_v,x_v+width_v,y_v+height_v])\n",
        "\n",
        "                cv2.rectangle(imx,(x_h,y_h), (x_h+width_h,y_h+height_h),(0,0,255),1)\n",
        "                cv2.rectangle(imx,(x_v,y_v), (x_v+width_v,y_v+height_v),(0,255,0),1)\n",
        "\n",
        "              #cv2.imwrite('horiz_vert'+str(num)+str(count+1)+'.jpg',imx)\n",
        "\n",
        "              #Non Max Suppresion\n",
        "              horiz_out = tf.image.non_max_suppression(\n",
        "                horiz_boxes,\n",
        "                probabilities,\n",
        "                max_output_size = 1000,\n",
        "                iou_threshold=0.1,\n",
        "                score_threshold=float('-inf'),\n",
        "                name=None\n",
        "              )\n",
        "              horiz_lines = np.sort(np.array(horiz_out))\n",
        "              im_nms = table_image_cv.copy()\n",
        "              for val in horiz_lines:\n",
        "                cv2.rectangle(im_nms, (int(horiz_boxes[val][0]),int(horiz_boxes[val][1])), (int(horiz_boxes[val][2]),int(horiz_boxes[val][3])),(0,0,255),1)\n",
        "\n",
        "              #cv2.imwrite('im_nms_h'+str(num)+str(count+1)+'.jpg',im_nms)\n",
        "\n",
        "              vert_out = tf.image.non_max_suppression(\n",
        "                vert_boxes,\n",
        "                probabilities,\n",
        "                max_output_size = 1000,\n",
        "                iou_threshold=0.1,\n",
        "                score_threshold=float('-inf'),\n",
        "                name=None\n",
        "              )\n",
        "              vert_lines = np.sort(np.array(vert_out))\n",
        "              im_nms = table_image_cv.copy()\n",
        "\n",
        "              for val in vert_lines:\n",
        "                cv2.rectangle(im_nms, (int(vert_boxes[val][0]),int(vert_boxes[val][1])), (int(vert_boxes[val][2]),int(vert_boxes[val][3])),(255,0,0),1)\n",
        "\n",
        "              #cv2.imwrite('im_nms_v'+str(num)+str(count+1)+'.jpg',im_nms)\n",
        "\n",
        "            # Create a directory for CSV files\n",
        "              csv_dir = os.path.join(pdf_output_dir, 'csv')\n",
        "              os.makedirs(csv_dir, exist_ok=True)\n",
        "\n",
        "            #Convert to CSV\n",
        "              out_array = [[\"\" for i in range(len(vert_lines))] for j in range(len(horiz_lines))]\n",
        "              # print(np.array(out_array).shape)\n",
        "              # print(out_array)\n",
        "\n",
        "              unordered_boxes = []\n",
        "\n",
        "              for i in vert_lines:\n",
        "                #print(vert_boxes[i])\n",
        "                unordered_boxes.append(vert_boxes[i][0])\n",
        "\n",
        "              ordered_boxes = np.argsort(unordered_boxes)\n",
        "\n",
        "              for i in range(len(horiz_lines)):\n",
        "                for j in range(len(vert_lines)):\n",
        "                  resultant = intersection(horiz_boxes[horiz_lines[i]], vert_boxes[vert_lines[ordered_boxes[j]]] )\n",
        "\n",
        "                  for b in range(len(boxes)):\n",
        "                    the_box = [boxes[b][0][0],boxes[b][0][1],boxes[b][2][0],boxes[b][2][1]]\n",
        "                    if(iou(resultant,the_box)>0.1):\n",
        "                      out_array[i][j] = texts[b]\n",
        "\n",
        "              print(counts)\n",
        "              # Create the CSV file path and name\n",
        "              for count in range(counts):\n",
        "                csv_filename = f'{pdf_name}_{page_num}_{count}.csv'\n",
        "                csv_path = os.path.join(csv_dir, csv_filename)\n",
        "                out_array = np.array(out_array)\n",
        "                pd.DataFrame(out_array).to_csv(csv_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Rough"
      ],
      "metadata": {
        "id": "sbh0hdsDOHs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_files = os.listdir(pdf_folder)\n",
        "for pdf_file in pdf_files:\n",
        "    if pdf_file.endswith('.pdf'):\n",
        "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
        "        pdf_name = os.path.splitext(pdf_file)[0]\n",
        "        print(pdf_name)\n",
        "        # Create a directory for PDF-specific data\n",
        "        pdf_output_dir = os.path.join(output_folder, pdf_name)\n",
        "        os.makedirs(pdf_output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "        # Convert PDF pages to images\n",
        "        pdf_images = convert_from_path(pdf_path)\n",
        "        page_dir = os.path.join(pdf_output_dir, pdf_name + '_pages')\n",
        "        os.makedirs(page_dir, exist_ok=True)\n",
        "\n",
        "        total_pages=0\n",
        "\n",
        "        for page_num, image in enumerate(pdf_images):\n",
        "            page_image_path = os.path.join(page_dir, f'page{page_num + 1}.jpg')\n",
        "            image.save(page_image_path, 'JPEG')\n",
        "            total_pages+=1\n",
        "\n",
        "            image = cv2.imread(page_image_path)\n",
        "            image = image[..., ::-1]\n",
        "            layout = model.detect(image)\n",
        "            counts=0\n",
        "            for l in layout:\n",
        "                if l.type == 'Table':\n",
        "                  counts +=1\n",
        "            #print(\"for page no.\", page_num+1,\" the table_count = \",counts)\n",
        "\n",
        "\n",
        "            #if tables are present in this page\n",
        "            if counts!=0:\n",
        "              #print(\"counts\")\n",
        "\n",
        "              # Create a directory for extracted table images\n",
        "              table_img_dir = os.path.join(pdf_output_dir, 'table_imgs')\n",
        "              os.makedirs(table_img_dir, exist_ok=True)\n",
        "\n",
        "              x_1=[0]*counts\n",
        "              y_1=[0]*counts\n",
        "              x_2=[0]*counts\n",
        "              y_2=[0]*counts\n",
        "              i=0\n",
        "              for l in layout:\n",
        "                #print(l)\n",
        "                if l.type == 'Table':\n",
        "                  x_1[i] = int(l.block.x_1)\n",
        "                  #print(l.block.x_1)\n",
        "                  y_1[i] = int(l.block.y_1)\n",
        "                  x_2[i] = int(l.block.x_2)\n",
        "                  y_2[i] = int(l.block.y_2)\n",
        "                  print(x_1[i],y_1[i],x_2[i],y_2[i])\n",
        "                  i+=1"
      ],
      "metadata": {
        "id": "HrBBoDNLudg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "              ocr = PaddleOCR(lang='en')\n",
        "              for count in range(counts):\n",
        "                  # Extract the table image\n",
        "                table_image = image[y_1[count]:y_2[count], x_1[count]:x_2[count]]\n",
        "\n",
        "                  # Save the table image with the specified naming format\n",
        "                table_img_path = os.path.join(table_img_dir, f'table_img_{page_num + 1}_{count + 1}.jpg')\n",
        "                cv2.imwrite(table_img_path, table_image)\n",
        "                #print(\"this ran\")"
      ],
      "metadata": {
        "id": "0h4hbz0_ufEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "              # Iterate through table images in 'table_imgs' directory\n",
        "              table_img_dir = os.path.join(pdf_output_dir, 'table_imgs')\n",
        "              table_images = [f for f in os.listdir(table_img_dir) if f.endswith('.jpg')]\n",
        "\n",
        "              for table_img_file in table_images:\n",
        "                  # Skip detection images\n",
        "                  if table_img_file.startswith('detections_'):\n",
        "                      continue\n",
        "\n",
        "                  table_img_path = os.path.join(table_img_dir, table_img_file)\n",
        "                  table_image_cv = cv2.imread(table_img_path)\n",
        "                  image_height = table_image_cv.shape[0]\n",
        "                  image_width = table_image_cv.shape[1]\n",
        "\n",
        "                  # Perform OCR on the table image\n",
        "                  output = ocr.ocr(table_img_path)[0]\n",
        "                  boxes = [line[0] for line in output]\n",
        "                  texts = [line[1][0] for line in output]\n",
        "                  probabilities = [line[1][1] for line in output]\n",
        "\n",
        "                  image_boxes = table_image_cv.copy()\n",
        "\n",
        "                  for box, text in zip(boxes, texts):\n",
        "                      cv2.rectangle(image_boxes, (int(box[0][0]), int(box[0][1])), (int(box[2][0]), int(box[2][1])), (0, 0, 255), 1)\n",
        "                      cv2.putText(image_boxes, text, (int(box[0][0]), int(box[0][1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (222, 0, 0), 1)\n",
        "\n",
        "                  detections_img_path = os.path.join(table_img_dir, f'detections_{table_img_file}')\n",
        "                  cv2.imwrite(detections_img_path, image_boxes)\n"
      ],
      "metadata": {
        "id": "PCZmRxaBv6mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "              imx = table_image_cv.copy()\n",
        "\n",
        "              horiz_boxes = []\n",
        "              vert_boxes = []\n",
        "\n",
        "              for box in boxes:\n",
        "                x_h, x_v = 0,int(box[0][0])\n",
        "                y_h, y_v = int(box[0][1]),0\n",
        "                width_h,width_v = image_width, int(box[2][0]-box[0][0])\n",
        "                height_h,height_v = int(box[2][1]-box[0][1]),image_height\n",
        "\n",
        "                horiz_boxes.append([x_h,y_h,x_h+width_h,y_h+height_h])\n",
        "                vert_boxes.append([x_v,y_v,x_v+width_v,y_v+height_v])\n",
        "\n",
        "                cv2.rectangle(imx,(x_h,y_h), (x_h+width_h,y_h+height_h),(0,0,255),1)\n",
        "                cv2.rectangle(imx,(x_v,y_v), (x_v+width_v,y_v+height_v),(0,255,0),1)\n",
        "\n",
        "              #cv2.imwrite('horiz_vert'+str(num)+str(count+1)+'.jpg',imx)\n"
      ],
      "metadata": {
        "id": "lNhLuwVwxTrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "              #Non Max Suppresion\n",
        "              horiz_out = tf.image.non_max_suppression(\n",
        "                horiz_boxes,\n",
        "                probabilities,\n",
        "                max_output_size = 1000,\n",
        "                iou_threshold=0.1,\n",
        "                score_threshold=float('-inf'),\n",
        "                name=None\n",
        "              )\n",
        "              horiz_lines = np.sort(np.array(horiz_out))\n",
        "              im_nms = image_cv.copy()\n",
        "              for val in horiz_lines:\n",
        "                cv2.rectangle(im_nms, (int(horiz_boxes[val][0]),int(horiz_boxes[val][1])), (int(horiz_boxes[val][2]),int(horiz_boxes[val][3])),(0,0,255),1)\n",
        "\n",
        "              #cv2.imwrite('im_nms_h'+str(num)+str(count+1)+'.jpg',im_nms)\n",
        "\n",
        "              vert_out = tf.image.non_max_suppression(\n",
        "                vert_boxes,\n",
        "                probabilities,\n",
        "                max_output_size = 1000,\n",
        "                iou_threshold=0.1,\n",
        "                score_threshold=float('-inf'),\n",
        "                name=None\n",
        "              )\n",
        "              vert_lines = np.sort(np.array(vert_out))\n",
        "              im_nms = table_image_cv.copy()\n",
        "\n",
        "              for val in vert_lines:\n",
        "                cv2.rectangle(im_nms, (int(vert_boxes[val][0]),int(vert_boxes[val][1])), (int(vert_boxes[val][2]),int(vert_boxes[val][3])),(255,0,0),1)\n",
        "\n",
        "              #cv2.imwrite('im_nms_v'+str(num)+str(count+1)+'.jpg',im_nms)\n"
      ],
      "metadata": {
        "id": "7HXQUUdnxVXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axUK4_ZiBHVG"
      },
      "source": [
        "#Modular Code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def intersection(box_1, box_2):\n",
        "  return [box_2[0], box_1[1],box_2[2], box_1[3]]"
      ],
      "metadata": {
        "id": "vEkqzdR6OZ9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iou(box_1, box_2):\n",
        "\n",
        "  x_1 = max(box_1[0], box_2[0])\n",
        "  y_1 = max(box_1[1], box_2[1])\n",
        "  x_2 = min(box_1[2], box_2[2])\n",
        "  y_2 = min(box_1[3], box_2[3])\n",
        "\n",
        "  inter = abs(max((x_2 - x_1, 0)) * max((y_2 - y_1), 0))\n",
        "  if inter == 0:\n",
        "      return 0\n",
        "\n",
        "  box_1_area = abs((box_1[2] - box_1[0]) * (box_1[3] - box_1[1]))\n",
        "  box_2_area = abs((box_2[2] - box_2[0]) * (box_2[3] - box_2[1]))\n",
        "\n",
        "  return inter / float(box_1_area + box_2_area - inter)"
      ],
      "metadata": {
        "id": "IWHqs_OIOVpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_table_images(table_images_dir, pdf_name, page_num):\n",
        "  ocr = PaddleOCR(lang='en')\n",
        "  table_images = [f for f in os.listdir(table_images_dir) if f.endswith('.jpg')]\n",
        "  print(table_images)\n",
        "\n",
        "  table_image_cv = cv2.imread(table_img_path)\n",
        "  if table_image_cv is None:\n",
        "    print(f\"Error reading image: {table_img_path}\")\n",
        "\n",
        "  # ... (Rest of your code to process table images and create CSV)\n",
        "\n",
        "\n",
        "  for table_img_file in table_images:\n",
        "    table_img_path = os.path.join(table_images_dir, table_img_file)\n",
        "    print(\"Table Image Path:\", table_img_path)\n",
        "\n",
        "    # Skip detection images\n",
        "    if table_img_file.startswith('detections_'):\n",
        "      continue\n",
        "\n",
        "    table_img_path = os.path.join(table_img_dir, table_img_file)\n",
        "    table_image_cv = cv2.imread(table_img_path)\n",
        "    image_height = table_image_cv.shape[0]\n",
        "    image_width = table_image_cv.shape[1]\n",
        "\n",
        "    # Perform OCR on the table image\n",
        "    output = ocr.ocr(table_img_path)[0]\n",
        "    boxes = [line[0] for line in output]\n",
        "    texts = [line[1][0] for line in output]\n",
        "    probabilities = [line[1][1] for line in output]\n",
        "\n",
        "    image_boxes = table_image_cv.copy()\n",
        "\n",
        "    for box, text in zip(boxes, texts):\n",
        "      cv2.rectangle(image_boxes, (int(box[0][0]), int(box[0][1])), (int(box[2][0]), int(box[2][1])), (0, 0, 255), 1)\n",
        "      cv2.putText(image_boxes, text, (int(box[0][0]), int(box[0][1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (222, 0, 0), 1)\n",
        "\n",
        "    detections_img_path = os.path.join(table_img_dir, f'detections_{table_img_file}')\n",
        "    cv2.imwrite(detections_img_path, image_boxes)\n",
        "\n",
        "  imx = table_image_cv.copy()\n",
        "\n",
        "  horiz_boxes = []\n",
        "  vert_boxes = []\n",
        "\n",
        "  for box in boxes:\n",
        "    x_h, x_v = 0,int(box[0][0])\n",
        "    y_h, y_v = int(box[0][1]),0\n",
        "    width_h,width_v = image_width, int(box[2][0]-box[0][0])\n",
        "    height_h,height_v = int(box[2][1]-box[0][1]),image_height\n",
        "\n",
        "    horiz_boxes.append([x_h,y_h,x_h+width_h,y_h+height_h])\n",
        "    vert_boxes.append([x_v,y_v,x_v+width_v,y_v+height_v])\n",
        "\n",
        "    cv2.rectangle(imx,(x_h,y_h), (x_h+width_h,y_h+height_h),(0,0,255),1)\n",
        "    cv2.rectangle(imx,(x_v,y_v), (x_v+width_v,y_v+height_v),(0,255,0),1)\n",
        "\n",
        "  #cv2.imwrite('horiz_vert'+str(num)+str(count+1)+'.jpg',imx)\n",
        "\n",
        "  #Non Max Suppresion\n",
        "  horiz_out = tf.image.non_max_suppression(\n",
        "    horiz_boxes,\n",
        "    probabilities,\n",
        "    max_output_size = 1000,\n",
        "    iou_threshold=0.1,\n",
        "    score_threshold=float('-inf'),\n",
        "    name=None\n",
        "  )\n",
        "  horiz_lines = np.sort(np.array(horiz_out))\n",
        "  im_nms = table_image_cv.copy()\n",
        "  for val in horiz_lines:\n",
        "    cv2.rectangle(im_nms, (int(horiz_boxes[val][0]),int(horiz_boxes[val][1])), (int(horiz_boxes[val][2]),int(horiz_boxes[val][3])),(0,0,255),1)\n",
        "\n",
        "  #cv2.imwrite('im_nms_h'+str(num)+str(count+1)+'.jpg',im_nms)\n",
        "\n",
        "  vert_out = tf.image.non_max_suppression(\n",
        "    vert_boxes,\n",
        "    probabilities,\n",
        "    max_output_size = 1000,\n",
        "    iou_threshold=0.1,\n",
        "    score_threshold=float('-inf'),\n",
        "    name=None\n",
        "  )\n",
        "  vert_lines = np.sort(np.array(vert_out))\n",
        "  im_nms = table_image_cv.copy()\n",
        "\n",
        "  for val in vert_lines:\n",
        "    cv2.rectangle(im_nms, (int(vert_boxes[val][0]),int(vert_boxes[val][1])), (int(vert_boxes[val][2]),int(vert_boxes[val][3])),(255,0,0),1)\n",
        "\n",
        "  #Convert to CSV\n",
        "  out_array = [[\"\" for i in range(len(vert_lines))] for j in range(len(horiz_lines))]\n",
        "  # print(np.array(out_array).shape)\n",
        "  # print(out_array)\n",
        "\n",
        "  unordered_boxes = []\n",
        "\n",
        "  for i in vert_lines:\n",
        "    #print(vert_boxes[i])\n",
        "    unordered_boxes.append(vert_boxes[i][0])\n",
        "\n",
        "  ordered_boxes = np.argsort(unordered_boxes)\n",
        "\n",
        "  for i in range(len(horiz_lines)):\n",
        "    for j in range(len(vert_lines)):\n",
        "      resultant = intersection(horiz_boxes[horiz_lines[i]], vert_boxes[vert_lines[ordered_boxes[j]]] )\n",
        "\n",
        "      for b in range(len(boxes)):\n",
        "        the_box = [boxes[b][0][0],boxes[b][0][1],boxes[b][2][0],boxes[b][2][1]]\n",
        "        if(iou(resultant,the_box)>0.1):\n",
        "          out_array[i][j] = texts[b]\n",
        "\n",
        "  print(counts)\n",
        "  # Create the CSV file path and name\n",
        "  for count in range(counts):\n",
        "    csv_filename = f'{pdf_name}_{page_num}_{count}.csv'\n",
        "    csv_path = os.path.join(csv_dir, csv_filename)\n",
        "    out_array = np.array(out_array)\n",
        "    pd.DataFrame(out_array).to_csv(csv_path)\n"
      ],
      "metadata": {
        "id": "U2EakFWPOygv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_table_images(layout, image, table_images_dir,pdf_name,page_num):\n",
        "# ... (Your code to extract and save table images)\n",
        "  counts=0\n",
        "  for l in layout:\n",
        "      if l.type == 'Table':\n",
        "        counts +=1\n",
        "  #print(\"for page no.\", page_num+1,\" the table_count = \",counts)\n",
        "\n",
        "\n",
        "  #if tables are present in this page\n",
        "  if counts!=0:\n",
        "\n",
        "    x_1=[0]*counts\n",
        "    y_1=[0]*counts\n",
        "    x_2=[0]*counts\n",
        "    y_2=[0]*counts\n",
        "    i=0\n",
        "    for l in layout:\n",
        "      #print(l)\n",
        "      if l.type == 'Table':\n",
        "        x_1[i] = int(l.block.x_1)\n",
        "        #print(l.block.x_1)\n",
        "        y_1[i] = int(l.block.y_1)\n",
        "        x_2[i] = int(l.block.x_2)\n",
        "        y_2[i] = int(l.block.y_2)\n",
        "        print(x_1[i],y_1[i],x_2[i],y_2[i])\n",
        "        i+=1\n",
        "\n",
        "    ocr = PaddleOCR(lang='en')\n",
        "    print(\"counts is:\",counts)\n",
        "    for count in range(counts):\n",
        "      # Extract the table image\n",
        "      table_image = image[y_1[count]:y_2[count], x_1[count]:x_2[count]]\n",
        "      print(\"is this running\")\n",
        "      # Save the table image with the specified naming format\n",
        "      table_img_path = os.path.join(table_images_dir, f'table_img_{page_num + 1}_{count + 1}.jpg')\n",
        "      cv2.imwrite(table_img_path, table_image)\n",
        "\n",
        "    process_table_images(table_images_dir, pdf_name, page_num)"
      ],
      "metadata": {
        "id": "0cOxdZGmOpAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_pdf(pdf_file, pdf_folder, output_folder):\n",
        "  pdf_name = os.path.splitext(pdf_file)[0]\n",
        "  print(pdf_name)\n",
        "  pdf_path = os.path.join(pdf_folder, pdf_file)\n",
        "\n",
        "  pdf_output_dir = os.path.join(output_folder, pdf_name)\n",
        "  os.makedirs(pdf_output_dir, exist_ok=True)\n",
        "\n",
        "  pdf_images = convert_from_path(pdf_path)\n",
        "  page_dir = os.path.join(pdf_output_dir, pdf_name + '_pages')\n",
        "  os.makedirs(page_dir, exist_ok=True)\n",
        "\n",
        "  table_images_dir = os.path.join(pdf_output_dir, 'table_imgs')\n",
        "  os.makedirs(table_images_dir, exist_ok=True)\n",
        "\n",
        "  # ... (Rest of the processing code)\n",
        "  total_pages=0\n",
        "\n",
        "  for page_num, image in enumerate(pdf_images):\n",
        "    page_image_path = os.path.join(page_dir, f'page{page_num + 1}.jpg')\n",
        "    image.save(page_image_path, 'JPEG')\n",
        "    total_pages+=1\n",
        "\n",
        "    image = cv2.imread(page_image_path)\n",
        "    image = image[..., ::-1]\n",
        "    layout = model.detect(image)\n",
        "\n",
        "    print(table_images_dir)\n",
        "    create_table_images(layout, image, table_images_dir,pdf_name,page_num)\n"
      ],
      "metadata": {
        "id": "EKrUkW3vOh2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_tables(pdf_folder, output_folder):\n",
        "  pdf_files = os.listdir(pdf_folder)\n",
        "  for pdf_file in pdf_files:\n",
        "      if pdf_file.endswith('.pdf'):\n",
        "          process_pdf(pdf_file, pdf_folder, output_folder)"
      ],
      "metadata": {
        "id": "oy-Tl-1gOdJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function\n",
        "def main():\n",
        "  pdf_folder = '/content/drive/MyDrive/5th Year (2023)/AmritanshuKhareDualDegree2024/MTP_Dataset_todo'  # Replace with your PDFs folder path\n",
        "  output_folder = '/content/drive/MyDrive/5th Year (2023)/AmritanshuKhareDualDegree2024/Test_Output_7'  # Replace with desired output folder path\n",
        "\n",
        "  extract_tables(pdf_folder, output_folder)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYYUvEYbVVMx",
        "outputId": "e7e5e2ed-0030-47e3-a146-3954fcc9f4b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "j.mtener.2020.100483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofxeWzzblgJN"
      },
      "outputs": [],
      "source": [
        "def extract_tables(pdf_folder, output_folder):\n",
        "  pdf_files = os.listdir(pdf_folder)\n",
        "  for pdf_file in pdf_files:\n",
        "    if pdf_file.endswith('.pdf'):\n",
        "        process_pdf(pdf_file, pdf_folder, output_folder)\n",
        "\n",
        "def process_pdf(pdf_file, pdf_folder, output_folder):\n",
        "  pdf_name = os.path.splitext(pdf_file)[0]\n",
        "  pdf_path = os.path.join(pdf_folder, pdf_file)\n",
        "\n",
        "  pdf_output_dir = os.path.join(output_folder, pdf_name)\n",
        "  os.makedirs(pdf_output_dir, exist_ok=True)\n",
        "\n",
        "  pdf_images = convert_from_path(pdf_path)\n",
        "  page_dir = os.path.join(pdf_output_dir, pdf_name + '_pages')\n",
        "  os.makedirs(page_dir, exist_ok=True)\n",
        "\n",
        "  # ... (Rest of the processing code)\n",
        "\n",
        "  table_images_dir = os.path.join(pdf_output_dir, 'table_imgs')\n",
        "  create_table_images(layout, image, table_images_dir)\n",
        "  process_table_images(table_images_dir, pdf_name, page_num)\n",
        "\n",
        "def create_table_images(layout, image, table_images_dir):\n",
        "    # ... (Your code to extract and save table images)\n",
        "\n",
        "def process_table_images(table_images_dir, pdf_name, page_num):\n",
        "  ocr = PaddleOCR(lang='en')\n",
        "  table_images = [f for f in os.listdir(table_images_dir) if f.endswith('.jpg')]\n",
        "\n",
        "  # ... (Rest of your code to process table images and create CSV)\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "  pdf_folder = '/content/drive/MyDrive/Dataset_todo'  # Replace with your PDFs folder path\n",
        "  output_folder = '/content/drive/MyDrive/Test_Output_6'  # Replace with desired output folder path\n",
        "\n",
        "  extract_tables(pdf_folder, output_folder)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfdDF5Jrmebx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEY/PYehISfkEhMKlS91UT"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}